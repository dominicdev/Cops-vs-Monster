{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominicdev/Cops-vs-Monster/blob/master/Research_Agent_Engine_DeepAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force --upgrade langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lna9eRVtblwk",
        "outputId": "28d01476-793f-4044-edc7-6918e5271142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting langsmith>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.4.31-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
            "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain)\n",
            "  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting packaging>=23.2 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
            "  Downloading charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
            "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting anyio (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
            "Downloading langsmith-0.4.31-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.3/386.3 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.9/444.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: zstandard, urllib3, typing-extensions, tenacity, sniffio, PyYAML, packaging, orjson, jsonpointer, idna, h11, greenlet, charset_normalizer, certifi, annotated-types, typing-inspection, SQLAlchemy, requests, pydantic-core, jsonpatch, httpcore, anyio, requests-toolbelt, pydantic, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: zstandard\n",
            "    Found existing installation: zstandard 0.25.0\n",
            "    Uninstalling zstandard-0.25.0:\n",
            "      Successfully uninstalled zstandard-0.25.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.5.0\n",
            "    Uninstalling tenacity-8.5.0:\n",
            "      Successfully uninstalled tenacity-8.5.0\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.1\n",
            "    Uninstalling sniffio-1.3.1:\n",
            "      Successfully uninstalled sniffio-1.3.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: orjson\n",
            "    Found existing installation: orjson 3.11.3\n",
            "    Uninstalling orjson-3.11.3:\n",
            "      Successfully uninstalled orjson-3.11.3\n",
            "  Attempting uninstall: jsonpointer\n",
            "    Found existing installation: jsonpointer 3.0.0\n",
            "    Uninstalling jsonpointer-3.0.0:\n",
            "      Successfully uninstalled jsonpointer-3.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 3.2.4\n",
            "    Uninstalling greenlet-3.2.4:\n",
            "      Successfully uninstalled greenlet-3.2.4\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.3\n",
            "    Uninstalling charset-normalizer-3.4.3:\n",
            "      Successfully uninstalled charset-normalizer-3.4.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.8.3\n",
            "    Uninstalling certifi-2025.8.3:\n",
            "      Successfully uninstalled certifi-2025.8.3\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.7.0\n",
            "    Uninstalling annotated-types-0.7.0:\n",
            "      Successfully uninstalled annotated-types-0.7.0\n",
            "  Attempting uninstall: typing-inspection\n",
            "    Found existing installation: typing-inspection 0.4.1\n",
            "    Uninstalling typing-inspection-0.4.1:\n",
            "      Successfully uninstalled typing-inspection-0.4.1\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.43\n",
            "    Uninstalling SQLAlchemy-2.0.43:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.43\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: jsonpatch\n",
            "    Found existing installation: jsonpatch 1.33\n",
            "    Uninstalling jsonpatch-1.33:\n",
            "      Successfully uninstalled jsonpatch-1.33\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.10.0\n",
            "    Uninstalling anyio-4.10.0:\n",
            "      Successfully uninstalled anyio-4.10.0\n",
            "  Attempting uninstall: requests-toolbelt\n",
            "    Found existing installation: requests-toolbelt 1.0.0\n",
            "    Uninstalling requests-toolbelt-1.0.0:\n",
            "      Successfully uninstalled requests-toolbelt-1.0.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.9\n",
            "    Uninstalling pydantic-2.11.9:\n",
            "      Successfully uninstalled pydantic-2.11.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.4.28\n",
            "    Uninstalling langsmith-0.4.28:\n",
            "      Successfully uninstalled langsmith-0.4.28\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.76\n",
            "    Uninstalling langchain-core-0.3.76:\n",
            "      Successfully uninstalled langchain-core-0.3.76\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.14.1 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.3 SQLAlchemy-2.0.43 annotated-types-0.7.0 anyio-4.11.0 certifi-2025.8.3 charset_normalizer-3.4.3 greenlet-3.2.4 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-core-0.3.76 langchain-text-splitters-0.3.11 langsmith-0.4.31 orjson-3.11.3 packaging-25.0 pydantic-2.11.9 pydantic-core-2.33.2 requests-2.32.5 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-extensions-4.15.0 typing-inspection-0.4.1 urllib3-2.5.0 zstandard-0.25.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "packaging"
                ]
              },
              "id": "f02a5161ce0c456bbe743f0c9258670d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/langchain-ai/deepagents.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nXEb6qk9x8N",
        "outputId": "855cfe62-551d-4acf-e47c-bfacfd2d6fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/langchain-ai/deepagents.git\n",
            "  Cloning https://github.com/langchain-ai/deepagents.git to /tmp/pip-req-build-2kipf8ox\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/langchain-ai/deepagents.git /tmp/pip-req-build-2kipf8ox\n",
            "  Resolved https://github.com/langchain-ai/deepagents.git to commit 57034628a04ec8bd61c08afe226faf92f567eb2c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langgraph>=1.0.0a3 (from deepagents==0.0.9)\n",
            "  Downloading langgraph-1.0.0a3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain-anthropic>=0.1.23 (from deepagents==0.0.9)\n",
            "  Downloading langchain_anthropic-0.3.20-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain>=1.0.0a9 (from deepagents==0.0.9)\n",
            "  Downloading langchain-1.0.0a9-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langgraph-prebuilt>=0.7.0a2 (from deepagents==0.0.9)\n",
            "  Downloading langgraph_prebuilt-0.7.0a2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain>=1.0.0a9->deepagents==0.0.9) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=0.3.11 in /usr/local/lib/python3.12/dist-packages (from langchain>=1.0.0a9->deepagents==0.0.9) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=1.0.0a9->deepagents==0.0.9) (2.11.9)\n",
            "Collecting anthropic<1,>=0.67.0 (from langchain-anthropic>=0.1.23->deepagents==0.0.9)\n",
            "  Downloading anthropic-0.68.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph>=1.0.0a3->deepagents==0.0.9)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph>=1.0.0a3->deepagents==0.0.9)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.0a3->deepagents==0.0.9) (3.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (4.15.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (0.4.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (6.0.3)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph>=1.0.0a3->deepagents==0.0.9)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph>=1.0.0a3->deepagents==0.0.9) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=1.0.0a9->deepagents==0.0.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=1.0.0a9->deepagents==0.0.9) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=1.0.0a9->deepagents==0.0.9) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic<1,>=0.67.0->langchain-anthropic>=0.1.23->deepagents==0.0.9) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain>=1.0.0a9->deepagents==0.0.9) (2.5.0)\n",
            "Downloading langchain-1.0.0a9-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_anthropic-0.3.20-py3-none-any.whl (31 kB)\n",
            "Downloading langgraph-1.0.0a3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.7.0a2-py3-none-any.whl (28 kB)\n",
            "Downloading anthropic-0.68.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepagents\n",
            "  Building wheel for deepagents (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepagents: filename=deepagents-0.0.9-py3-none-any.whl size=26766 sha256=09ed3bb6b6ddf259fa277b0b4512f312a560dc1d601a97de861dc962bbbf61ad\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j26oskux/wheels/75/5c/d3/55dc466703ee4fc8ca49e1e8ced160eb589f0592de8a6eb77a\n",
            "Successfully built deepagents\n",
            "Installing collected packages: ormsgpack, langgraph-sdk, anthropic, langgraph-checkpoint, langchain-anthropic, langgraph-prebuilt, langgraph, langchain, deepagents\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed anthropic-0.68.0 deepagents-0.0.9 langchain-1.0.0a9 langchain-anthropic-0.3.20 langgraph-1.0.0a3 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.7.0a2 langgraph-sdk-0.2.9 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph \\\n",
        "  langchain \\\n",
        "  langchain-google-vertexai \\\n",
        "  google-cloud-aiplatform \\\n",
        "  vertexai \\\n",
        "  langchain-community \\\n",
        "  requests \\\n",
        "  langchain_openai \\\n",
        "  tavily-python \\\n",
        "  deepagents \\\n",
        "  langchain-google-genai \\\n",
        "  google-cloud-aiplatform[agent_engines,langgraph]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PeG9kXVfTUp1",
        "outputId": "56fbff88-651d-4b48-b496-b192e79c1280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.0a3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.0a9)\n",
            "Collecting langchain-google-vertexai\n",
            "  Downloading langchain_google_vertexai-2.1.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.12/dist-packages (1.114.0)\n",
            "Collecting vertexai\n",
            "  Downloading vertexai-1.71.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.7.12-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: deepagents in /usr/local/lib/python3.12/dist-packages (0.0.9)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.76)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt==0.7.0a2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.7.0a2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=0.3.11 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.18 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (2.19.0)\n",
            "Requirement already satisfied: httpx<1,>=0.28 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse<1,>=0.4 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (0.4.1)\n",
            "Collecting validators<1,>=0.22 (from langchain-google-vertexai)\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: bottleneck<2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (1.4.2)\n",
            "Requirement already satisfied: numexpr<3,>=2.8.6 in /usr/local/lib/python3.12/dist-packages (from langchain-google-vertexai) (2.12.1)\n",
            "Collecting pyarrow<22,>=19.0.1 (from langchain-google-vertexai)\n",
            "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (5.29.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (25.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.38.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.15.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "INFO: pip is looking at multiple versions of vertexai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting vertexai\n",
            "  Downloading vertexai-1.71.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.70.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.69.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.68.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.67.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.67.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.66.0-py3-none-any.whl.metadata (10 kB)\n",
            "INFO: pip is still looking at multiple versions of vertexai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading vertexai-1.65.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.64.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.63.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.60.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.49.0-py3-none-any.whl.metadata (10 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading vertexai-1.48.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading vertexai-1.47.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "  Downloading vertexai-1.46.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "  Downloading vertexai-1.43.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.31)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.108.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: langchain-anthropic>=0.1.23 in /usr/local/lib/python3.12/dist-packages (from deepagents) (0.3.20)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting filetype<2,>=1.2 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "\u001b[33mWARNING: google-cloud-aiplatform 1.114.0 does not provide the extra 'langgraph'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: cloudpickle<4.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[agent_engines,langgraph]) (3.1.1)\n",
            "Requirement already satisfied: google-cloud-trace<2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[agent_engines,langgraph]) (1.16.2)\n",
            "Requirement already satisfied: google-cloud-logging<4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[agent_engines,langgraph]) (3.12.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[agent_engines,langgraph]) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-gcp-trace<2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[agent_engines,langgraph]) (1.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Requirement already satisfied: google-cloud-appengine-logging<2.0.0,>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent_engines,langgraph]) (1.6.2)\n",
            "Requirement already satisfied: google-cloud-audit-log<1.0.0,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent_engines,langgraph]) (0.3.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.12.4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent_engines,langgraph]) (0.14.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent_engines,langgraph]) (1.37.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3,>=2.18->langchain-google-vertexai) (1.7.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (4.11.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.28->langchain-google-vertexai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.28->langchain-google-vertexai) (0.16.0)\n",
            "Requirement already satisfied: anthropic<1,>=0.67.0 in /usr/local/lib/python3.12/dist-packages (from langchain-anthropic>=0.1.23->deepagents) (0.68.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: opentelemetry-resourcedetector-gcp==1.*,>=1.5.0dev0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-gcp-trace<2->google-cloud-aiplatform[agent_engines,langgraph]) (1.9.0a0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<2->google-cloud-aiplatform[agent_engines,langgraph]) (0.58b0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.9.0->google-cloud-logging<4->google-cloud-aiplatform[agent_engines,langgraph]) (8.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.9.0->google-cloud-logging<4->google-cloud-aiplatform[agent_engines,langgraph]) (3.23.0)\n",
            "Downloading langchain_google_vertexai-2.1.2-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vertexai-1.43.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tavily_python-0.7.12-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, validators, pyarrow, mypy-extensions, marshmallow, typing-inspect, tavily-python, dataclasses-json, langchain_openai, google-ai-generativelanguage, langchain-google-genai, vertexai, langchain-google-vertexai, langchain-community\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.7.0 langchain-community-0.3.29 langchain-google-genai-2.1.12 langchain-google-vertexai-2.1.2 langchain_openai-0.3.33 marshmallow-3.26.1 mypy-extensions-1.1.0 pyarrow-21.0.0 tavily-python-0.7.12 typing-inspect-0.9.0 validators-0.35.0 vertexai-1.43.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "cad312c892904f3a9ac0298b6ecaec40"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b169fec-da38-42e9-a0d4-53cdd40543c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"dom-fireact\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "STAGING_BUCKET = \"gs://langgraph_agent_engine_bucket\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Building and deploying a LangGraph app on Agent Engine\n",
        "\n",
        "In the following sections, we'll walk through the process of building and deploying a LangGraph application using Agent Engine in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b3004f33544"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Import the necessary Python libraries. These libraries provide the tools we need to interact with LangGraph, Vertex AI, and other components of our application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67acfba81563"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "import os\n",
        "from typing import TypedDict, List, Dict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "# --- DeepAgents imports ---\n",
        "from deepagents.middleware import PlanningMiddleware, FilesystemMiddleware, SubAgentMiddleware\n",
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import SummarizationMiddleware\n",
        "\n",
        "from vertexai import agent_engines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "TAVILY_API_KEY=os.environ[\"TAVILY_API_KEY\"]"
      ],
      "metadata": {
        "id": "Nw-Nj5st3Rs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "pzT0Ya3IBYF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "w7QMfuUc4Bcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tavily_search(query: str, max_results: int = 5) -> str:\n",
        "    \"\"\"Run Tavily search and return formatted results\"\"\"\n",
        "    tavily_api = TavilySearchAPIWrapper()\n",
        "    tavily_tool = TavilySearchResults(api_wrapper=tavily_api, max_results=max_results)\n",
        "    tav_res = tavily_tool.run(query)\n",
        "    # print(\"call tavily search\",tav_res)\n",
        "    return tav_res\n"
      ],
      "metadata": {
        "id": "B617TdOwBfY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sub agent and middleware"
      ],
      "metadata": {
        "id": "dvyjq4Llg_3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# Prompts (from deepagents/examples/research/research_agent.py)\n",
        "# ===========================================================\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "sub_research_prompt = \"\"\"You are a dedicated researcher. Your job is to conduct research based on the users questions.\n",
        "\n",
        "Conduct thorough research and then reply to the user with a detailed answer to their question.\n",
        "\n",
        "Only your FINAL answer will be passed on to the user. They will have NO knowledge of anything except your final message, so your final report should be your final message!\n",
        "\"\"\"\n",
        "\n",
        "sub_critique_prompt = \"\"\"You are a dedicated editor. You are being tasked to critique a report.\n",
        "\n",
        "The user may ask for specific areas to critique the report in. Respond to the user with a detailed critique of the report. Things that could be improved.\n",
        "\n",
        "Do not write the final report yourself.\n",
        "\n",
        "Things to check:\n",
        "- Check that each section is appropriately named\n",
        "- Check that the report is written as you would find in an essay or a textbook - it should be text heavy, not just bullet points\n",
        "- Check that the report is comprehensive and covers all key areas\n",
        "- Check for analysis of causes, impacts, and trends\n",
        "- Check that the article closely follows the research topic\n",
        "- Ensure clear structure, fluent language, and readability\n",
        "\"\"\"\n",
        "\n",
        "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.\n",
        "\n",
        "The first thing you should do is to note the original user question for reference.\n",
        "\n",
        "Use the research-agent to conduct deep research. It will respond to your questions/topics with a detailed answer.\n",
        "\n",
        "When you have enough information to write a final report, draft it.\n",
        "\n",
        "You can call the critique-agent to get a critique of the draft. After that (if needed) you can do more research and edit the draft.\n",
        "\n",
        "CRITICAL: Make sure the answer is written in the same language as the human messages!\n",
        "\n",
        "Please create a detailed answer to the overall research brief that:\n",
        "1. Is well-organized with proper headings (#, ##, ###)\n",
        "2. Includes specific facts and insights\n",
        "3. References relevant sources using [Title](URL) format\n",
        "4. Provides a balanced, thorough analysis\n",
        "5. Includes a \"Sources\" section at the end with all referenced links\n",
        "\n",
        "Format the report in clear markdown with proper structure.\n",
        "\"\"\"\n",
        "\n",
        "class AgentsResponse(BaseModel):\n",
        "    \"\"\"Represents the structured response from the agents.\"\"\"\n",
        "    research_response: str = Field(description=\"The detailed research conducted by the research agent.\")\n",
        "    critique_response: str = Field(description=\"The critique of the draft report provided by the critique agent.\")\n",
        "    final_report: str = Field(description=\"The final, polished report based on the research and critique.\")\n",
        "\n",
        "\n",
        "# ===========================================================\n",
        "# Build Agent with Middleware\n",
        "# ===========================================================\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")\n",
        "def make_deep_research_agent():\n",
        "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    tools = [tavily_search]\n",
        "\n",
        "    # Subagents\n",
        "    research_sub_agent = {\n",
        "        \"name\": \"research-agent\",\n",
        "        \"description\": \"Used to research more in depth questions.\",\n",
        "        \"prompt\": sub_research_prompt,\n",
        "        \"tools\": tools,\n",
        "    }\n",
        "    critique_sub_agent = {\n",
        "        \"name\": \"critique-agent\",\n",
        "        \"description\": \"Used to critique the final report.\",\n",
        "        \"prompt\": sub_critique_prompt,\n",
        "    }\n",
        "\n",
        "    middleware = [\n",
        "        PlanningMiddleware(),\n",
        "        FilesystemMiddleware(),\n",
        "        SubAgentMiddleware(\n",
        "            default_subagent_tools=tools,\n",
        "            subagents=[research_sub_agent, critique_sub_agent],\n",
        "            model=model,\n",
        "            is_async=True,\n",
        "        ),\n",
        "        SummarizationMiddleware(model=model),\n",
        "    ]\n",
        "\n",
        "    agent = create_agent(\n",
        "        model,\n",
        "        prompt=research_instructions,\n",
        "        tools=tools,\n",
        "        middleware=middleware,\n",
        "        # response_format=AgentsResponse\n",
        "\n",
        "    )\n",
        "    return agent"
      ],
      "metadata": {
        "id": "lUPfrw1IBzN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Agent"
      ],
      "metadata": {
        "id": "EFKA5y85hBzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LangGraphResearchAgent:\n",
        "    def __init__(self, project: str, location: str):\n",
        "        self.project_id = project\n",
        "        self.location = location\n",
        "        self.runnable = None\n",
        "\n",
        "    def set_up(self):\n",
        "        # deep_agent = make_deep_research_agent()\n",
        "        # self.runnable = deep_agent\n",
        "\n",
        "        pass\n",
        "\n",
        "    async def query(self, inputs: dict) -> dict:\n",
        "      # result = await app.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": f\"search me about this company{inputs}\"}]})\n",
        "        deep_agent = make_deep_research_agent()\n",
        "\n",
        "        # self.runnable = deep_agent\n",
        "        return await deep_agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": f\"search me about this company {inputs}\"}]})"
      ],
      "metadata": {
        "id": "YQKRI2jMBu2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing"
      ],
      "metadata": {
        "id": "LovieKiX7Nhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "PROJECT_ID = \"dom-fireact\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "STAGING_BUCKET = \"gs://langgraph_agent_engine_bucket\"  # @param {type:\"string\"}\n",
        "\n",
        "agent = LangGraphResearchAgent(project=PROJECT_ID, location=LOCATION)\n",
        "agent.set_up()\n",
        "\n",
        "inputs = {\n",
        "    \"project_title\": \"LexisNexis\",\n",
        "    \"guests\": [\"Min Chen\"],\n",
        "    \"user_company\": \"LexisNexis\",\n",
        "    \"include_company_research\": True,\n",
        "    \"interview_angle\": \"How they are helping legal firms with Gen-AI and LLMs\",\n",
        "    \"user_questions\": [\"How do you choose which LLMs to use for legal products?\"],\n",
        "    \"additional_keywords\": [\"LexisNexis products\"],\n",
        "    \"outputs\": {},\n",
        "}\n",
        "\n",
        "result = await agent.query(inputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "EMlYQ5WkCKHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4da453a-7648-4991-96ae-352e73ae66b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1515401711.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  tavily_tool = TavilySearchResults(api_wrapper=tavily_api, max_results=max_results)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for msg in result[\"messages\"]:\n",
        "    msg.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbty4gzzPwir",
        "outputId": "ad545f4c-2b3f-4b2c-fa5d-c843b7bea076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "search me about this company {'project_title': 'LexisNexis', 'guests': ['Min Chen'], 'user_company': 'LexisNexis', 'include_company_research': True, 'interview_angle': 'How they are helping legal firms with Gen-AI and LLMs', 'user_questions': ['How do you choose which LLMs to use for legal products?'], 'additional_keywords': ['LexisNexis products'], 'outputs': {}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  task (call_ooIatTJfwc3HJ6Vj2J9ExkGT)\n",
            " Call ID: call_ooIatTJfwc3HJ6Vj2J9ExkGT\n",
            "  Args:\n",
            "    description: Conduct thorough research on LexisNexis, focusing on how they are helping legal firms with Generative AI (Gen-AI) and Large Language Models (LLMs). Include information on their products, how they choose which LLMs to use for legal products, and any relevant insights or case studies. Provide a comprehensive report with specific facts and insights, and reference relevant sources.\n",
            "    subagent_type: research-agent\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: task\n",
            "\n",
            "LexisNexis is at the forefront of integrating Generative AI (Gen-AI) and Large Language Models (LLMs) into the legal industry, providing innovative solutions that enhance productivity and efficiency for legal firms. Here’s a comprehensive overview of how LexisNexis is leveraging these technologies, the products they offer, their selection criteria for LLMs, and relevant case studies.\n",
            "\n",
            "### Overview of LexisNexis and Generative AI\n",
            "\n",
            "LexisNexis has been actively developing AI-driven solutions since 2018, experimenting with various models, including those from Google and earlier versions of GPT. The company recognizes that the legal field is fundamentally language-centric, making it a prime candidate for the application of LLMs and Gen-AI technologies. Their approach aims to harness the potential of these tools while addressing the risks associated with early-generation AI models, such as those seen in open web tools like ChatGPT.\n",
            "\n",
            "### Key Products\n",
            "\n",
            "1. **Lexis+ AI**: This is LexisNexis's flagship Gen-AI platform, designed to transform legal work through advanced capabilities in legal research, drafting, and summarization. It provides users with access to the world's largest repository of legal content, ensuring that the information generated is accurate and reliable. Lexis+ AI is equipped with features that allow legal professionals to save time on routine tasks, enabling them to focus on more complex legal work.\n",
            "\n",
            "2. **LexisNexis Protégé**: This AI assistant enhances collaboration between legal professionals and AI, amplifying productivity and strategic impact. It is designed to assist in various legal tasks, making it easier for firms to integrate AI into their workflows.\n",
            "\n",
            "### Selection of LLMs for Legal Products\n",
            "\n",
            "LexisNexis employs a deliberate product development strategy when selecting LLMs for their legal products. They focus on models that are specifically tailored for the legal domain, ensuring that the tools they provide are not only innovative but also secure and reliable. The company emphasizes the importance of data security and privacy, which are critical in the legal field. By leveraging advanced LLMs, LexisNexis aims to mitigate risks while maximizing the benefits of Gen-AI technologies.\n",
            "\n",
            "### Insights and Case Studies\n",
            "\n",
            "1. **Productivity Gains**: A survey conducted by LexisNexis revealed that 47% of legal professionals believe Gen-AI enables new business opportunities, while 90% expect their investment in these technologies to increase over the next five years. This indicates a strong belief in the transformative potential of Gen-AI within the legal sector.\n",
            "\n",
            "2. **Case Studies**: LexisNexis has documented several case studies showcasing how firms like Gibbons and Rupp Pfalzgraf have successfully integrated Lexis+ AI into their operations. These firms reported increased caseloads and enhanced analytical capabilities, allowing them to deliver exceptional value to clients in a rapidly evolving legal landscape.\n",
            "\n",
            "3. **Adoption Trends**: A recent survey of over 1,200 legal professionals in the UK indicated a significant rise in the use of generative AI tools, from 11% in July 2023 to 26% in January 2024. This trend highlights the growing acceptance and reliance on AI technologies in legal practices.\n",
            "\n",
            "4. **Operational Changes**: Many firms have made operational adjustments due to the integration of Gen-AI, including launching AI-powered products for internal use and providing AI-related training for staff. This reflects a broader shift in how legal firms are adapting to technological advancements.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "LexisNexis is leading the charge in the legal industry's adoption of Generative AI and LLMs, providing tools that enhance productivity, improve accuracy, and create new business opportunities. Their strategic approach to product development, combined with a focus on security and reliability, positions them as a key player in the future of legal technology. As the legal landscape continues to evolve, LexisNexis's innovations will likely play a crucial role in shaping how legal professionals operate.\n",
            "\n",
            "For further details, you can explore the following resources:\n",
            "- [Lexis+ AI Overview](https://www.lexisnexis.com/en-us/products/lexis-plus-ai.page)\n",
            "- [Tech Innovation with LLMs](https://www.lexisnexis.com/community/insights/legal/b/thought-leadership/posts/tech-innovation-with-llms-producing-more-secure-and-reliable-gen-ai-results)\n",
            "- [Legal Gen AI Use Cases](https://www.lexisnexis.com/en-us/products/lexis-plus-ai/thought-leadership.page)\n",
            "- [Lawyers Gear Up for Generative AI](https://www.lexisnexis.co.uk/insights/lawyers-cross-into-the-new-era-of-generative-ai/index.html)\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "# LexisNexis and Generative AI: Enhancing Legal Practices\n",
            "\n",
            "## Overview of LexisNexis and Generative AI\n",
            "\n",
            "LexisNexis is a leading provider of legal research and information services, actively integrating Generative AI (Gen-AI) and Large Language Models (LLMs) into the legal industry. Since 2018, the company has been developing AI-driven solutions that enhance productivity and efficiency for legal firms. Recognizing that the legal field is fundamentally language-centric, LexisNexis aims to harness the potential of these technologies while addressing the associated risks.\n",
            "\n",
            "## Key Products\n",
            "\n",
            "### 1. Lexis+ AI\n",
            "Lexis+ AI is the flagship Gen-AI platform of LexisNexis, designed to transform legal work through advanced capabilities in legal research, drafting, and summarization. It provides users with access to the world's largest repository of legal content, ensuring that the information generated is accurate and reliable. Key features include:\n",
            "- Time-saving tools for routine tasks.\n",
            "- Enhanced focus on complex legal work.\n",
            "\n",
            "### 2. LexisNexis Protégé\n",
            "This AI assistant enhances collaboration between legal professionals and AI, amplifying productivity and strategic impact. It is designed to assist in various legal tasks, making it easier for firms to integrate AI into their workflows.\n",
            "\n",
            "## Selection of LLMs for Legal Products\n",
            "\n",
            "LexisNexis employs a deliberate product development strategy when selecting LLMs for their legal products. They focus on models specifically tailored for the legal domain, ensuring that the tools provided are innovative, secure, and reliable. Key considerations include:\n",
            "- Data security and privacy, critical in the legal field.\n",
            "- Mitigating risks while maximizing the benefits of Gen-AI technologies.\n",
            "\n",
            "## Insights and Case Studies\n",
            "\n",
            "### 1. Productivity Gains\n",
            "A survey conducted by LexisNexis revealed that 47% of legal professionals believe Gen-AI enables new business opportunities, while 90% expect their investment in these technologies to increase over the next five years. This indicates a strong belief in the transformative potential of Gen-AI within the legal sector.\n",
            "\n",
            "### 2. Case Studies\n",
            "LexisNexis has documented several case studies showcasing how firms like Gibbons and Rupp Pfalzgraf have successfully integrated Lexis+ AI into their operations. These firms reported:\n",
            "- Increased caseloads.\n",
            "- Enhanced analytical capabilities, allowing them to deliver exceptional value to clients.\n",
            "\n",
            "### 3. Adoption Trends\n",
            "A recent survey of over 1,200 legal professionals in the UK indicated a significant rise in the use of generative AI tools, from 11% in July 2023 to 26% in January 2024. This trend highlights the growing acceptance and reliance on AI technologies in legal practices.\n",
            "\n",
            "### 4. Operational Changes\n",
            "Many firms have made operational adjustments due to the integration of Gen-AI, including:\n",
            "- Launching AI-powered products for internal use.\n",
            "- Providing AI-related training for staff.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "LexisNexis is leading the charge in the legal industry's adoption of Generative AI and LLMs, providing tools that enhance productivity, improve accuracy, and create new business opportunities. Their strategic approach to product development, combined with a focus on security and reliability, positions them as a key player in the future of legal technology. As the legal landscape continues to evolve, LexisNexis's innovations will likely play a crucial role in shaping how legal professionals operate.\n",
            "\n",
            "## Sources\n",
            "- [Lexis+ AI Overview](https://www.lexisnexis.com/en-us/products/lexis-plus-ai.page)\n",
            "- [Tech Innovation with LLMs](https://www.lexisnexis.com/community/insights/legal/b/thought-leadership/posts/tech-innovation-with-llms-producing-more-secure-and-reliable-gen-ai-results)\n",
            "- [Legal Gen AI Use Cases](https://www.lexisnexis.com/en-us/products/lexis-plus-ai/thought-leadership.page)\n",
            "- [Lawyers Gear Up for Generative AI](https://www.lexisnexis.co.uk/insights/lawyers-cross-into-the-new-era-of-generative-ai/index.html)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy to agent engine"
      ],
      "metadata": {
        "id": "JfDV5EmchE89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai import agent_engines\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "remote_agent = agent_engines.create(\n",
        "    LangGraphResearchAgent (project=PROJECT_ID, location=LOCATION),\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[agent_engines,langchain]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.11.2\",\n",
        "        \"langgraph\",\n",
        "        \"httpx\",\n",
        "        \"langchain-community\",\n",
        "        \"vertexai\",\n",
        "        \"langchain_openai\",\n",
        "        \"tavily-python\",\n",
        "        \"deepagents\",\n",
        "        \"git+https://github.com/langchain-ai/deepagents.git\",\n",
        "\n",
        "    ],\n",
        "    display_name=\"Research Agent Engine with LangGraph\",\n",
        "    description=\"This is a Research Agent Engine that uses LangGraph\",\n",
        "    extra_packages=[],\n",
        "    env_vars={   # 👈 add this\n",
        "        \"TAVILY_API_KEY\": TAVILY_API_KEY,\n",
        "        \"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"],\n",
        "        'GOOGLE_API_KEY': os.environ[\"GOOGLE_API_KEY\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_7zjISj5Eib",
        "outputId": "1527161b-9703-4c79-a5a3-3d576c5ef97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.agent_engines:Identified the following requirements: {'pydantic': '2.11.2', 'cloudpickle': '3.0.0'}\n",
            "WARNING:vertexai.agent_engines:Failed to parse constraint: git+https://github.com/langchain-ai/deepagents.git. Exception: Expected end or semicolon (after name and no valid version specifier)\n",
            "    git+https://github.com/langchain-ai/deepagents.git\n",
            "       ^\n",
            "INFO:vertexai.agent_engines:The final list of requirements: ['google-cloud-aiplatform[agent_engines,langchain]', 'cloudpickle==3.0.0', 'pydantic==2.11.2', 'langgraph', 'httpx', 'langchain-community', 'vertexai', 'langchain_openai', 'tavily-python', 'deepagents', 'git+https://github.com/langchain-ai/deepagents.git']\n",
            "INFO:vertexai.agent_engines:Using bucket langgraph_agent_engine_bucket\n",
            "INFO:vertexai.agent_engines:Wrote to gs://langgraph_agent_engine_bucket/agent_engine/agent_engine.pkl\n",
            "INFO:vertexai.agent_engines:Writing to gs://langgraph_agent_engine_bucket/agent_engine/requirements.txt\n",
            "INFO:vertexai.agent_engines:Creating in-memory tarfile of extra_packages\n",
            "INFO:vertexai.agent_engines:Writing to gs://langgraph_agent_engine_bucket/agent_engine/dependencies.tar.gz\n",
            "INFO:vertexai.agent_engines:Creating AgentEngine\n",
            "INFO:vertexai.agent_engines:Create AgentEngine backing LRO: projects/248711292256/locations/us-central1/reasoningEngines/6542204136429977600/operations/7257700418988277760\n",
            "INFO:vertexai.agent_engines:View progress and logs at https://console.cloud.google.com/logs/query?project=dom-fireact\n",
            "INFO:vertexai.agent_engines:AgentEngine created. Resource name: projects/248711292256/locations/us-central1/reasoningEngines/6542204136429977600\n",
            "INFO:vertexai.agent_engines:To use this AgentEngine in another session:\n",
            "INFO:vertexai.agent_engines:agent_engine = vertexai.agent_engines.get('projects/248711292256/locations/us-central1/reasoningEngines/6542204136429977600')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test agent engine"
      ],
      "metadata": {
        "id": "GXd9QR6RhI-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_query = {\n",
        "    \"project_title\": \"LexisNexis\",\n",
        "    \"guests\": [\"Min Chen\"],\n",
        "    \"user_company\": \"LexisNexis\",\n",
        "    \"include_company_research\": True,\n",
        "    \"interview_angle\": \"How they are helping legal firms with Gen-AI and LLMs\",\n",
        "    \"user_questions\": [\n",
        "        \"How do you choose which LLMs to use for legal products?\",\n",
        "        \"How do you get over the fact of not knowing what they were trained on?\",\n",
        "        \"In law, we've had lots of different problems with hallucinations and lawyers citing cases that didn't exist. How do you deal with this kind of thing?\",\n",
        "    ],\n",
        "    \"additional_keywords\": [\"LexisNexis products\"],\n",
        "    \"outputs\": {},\n",
        "\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "6nakjhggHxvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = remote_agent.query(inputs=state_query)"
      ],
      "metadata": {
        "id": "8FUpClsJMT8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['messages'][-1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QwrZpEFb1fx",
        "outputId": "87aace69-6861-4f76-e715-31789b98a691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# LexisNexis and Its Role in Legal AI\n",
            "\n",
            "## Overview\n",
            "LexisNexis is a leading provider of legal research and information services, actively integrating Generative AI (Gen-AI) and Large Language Models (LLMs) into its offerings. This report explores how LexisNexis is helping legal firms leverage these technologies, focusing on their products, selection processes for LLMs, and strategies for addressing challenges such as hallucinations in legal contexts.\n",
            "\n",
            "## 1. Products and Offerings\n",
            "LexisNexis has developed several AI-driven products designed to enhance legal research and practice:\n",
            "\n",
            "### Lexis+ AI\n",
            "- **Features**: This platform incorporates Gen-AI capabilities, offering conversational search, document drafting, summarization, and document analysis.\n",
            "- **Personalized AI Assistant**: The platform includes **Protégé™**, an AI assistant that aids lawyers in navigating legal information more efficiently.\n",
            "\n",
            "### Agentic AI\n",
            "- **Concept**: LexisNexis is focusing on building AI systems that can autonomously execute plans and reflect on their actions.\n",
            "- **Goals**: The aim is to enhance speed and reduce costs while ensuring that the AI is grounded in trusted legal knowledge.\n",
            "\n",
            "## 2. Selection Process for LLMs\n",
            "The selection of LLMs at LexisNexis involves several critical considerations:\n",
            "\n",
            "### Data Provenance\n",
            "- **Quality Assurance**: LexisNexis emphasizes the importance of using high-quality, reliable training data sourced from trusted legal materials, which builds user confidence in the accuracy of AI outputs.\n",
            "\n",
            "### Customization and Fine-Tuning\n",
            "- **Legal Context Adaptation**: The company focuses on fine-tuning models to better understand legal terminology and nuances, ensuring relevance in legal applications.\n",
            "\n",
            "## 3. Handling Training Data Uncertainties\n",
            "LexisNexis addresses uncertainties related to training data through:\n",
            "\n",
            "### Rigorous Data Management\n",
            "- **Confidence in Data**: The company maintains a high level of confidence in the data used for training, ensuring it is relevant and accurate, which is crucial in legal contexts.\n",
            "\n",
            "### Continuous Improvement\n",
            "- **Ongoing Evaluation**: LexisNexis is committed to continuously evaluating and enhancing their models, updating training datasets to reflect the latest legal developments and case law.\n",
            "\n",
            "## 4. Addressing Hallucinations in Legal Contexts\n",
            "Hallucinations, or the generation of incorrect or misleading information by AI, pose significant challenges in legal applications. LexisNexis tackles this issue by:\n",
            "\n",
            "### Implementing Safeguards\n",
            "- **Cross-Referencing Outputs**: The company is developing mechanisms to minimize hallucinations, such as cross-referencing AI outputs with verified legal sources and providing users with context about the AI's reasoning.\n",
            "\n",
            "### User Education\n",
            "- **Educating Legal Professionals**: LexisNexis emphasizes the importance of educating legal professionals on the capabilities and limitations of Gen-AI, fostering a cautious approach to AI-generated content.\n",
            "\n",
            "## 5. Min Chen's Role and Contributions\n",
            "Min Chen serves as the Chief AI Officer at LexisNexis and plays a pivotal role in shaping the company's Gen-AI strategy. Her contributions include:\n",
            "\n",
            "### Vision for Agentic AI\n",
            "- **Advocacy**: Chen promotes the development of agentic AI systems that can enhance legal workflows by autonomously carrying out tasks and providing intelligent insights.\n",
            "\n",
            "### Focus on Accuracy and Trust\n",
            "- **Building Trustworthy Systems**: She emphasizes the need for AI systems that are efficient and trustworthy, ensuring that legal professionals can rely on AI-generated outputs.\n",
            "\n",
            "### Public Engagement\n",
            "- **Sharing Insights**: Through interviews and discussions, Chen shares insights on the future of AI in the legal industry, highlighting the need for continuous innovation and adaptation to meet the evolving needs of legal practitioners.\n",
            "\n",
            "## Conclusion\n",
            "LexisNexis is at the forefront of integrating Generative AI and LLMs into the legal sector, focusing on enhancing efficiency, accuracy, and trustworthiness in legal services. Their strategic approach, led by figures like Min Chen, aims to address the unique challenges posed by AI in legal contexts while leveraging its potential to transform legal practice.\n",
            "\n",
            "## Sources\n",
            "- [LexisNexis Products](https://www.lexisnexis.com/en-us/products.page)\n",
            "- [Min Chen's Contributions](https://www.lexisnexis.com/en-us/about/leadership.page)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "After you've finished experimenting, it's a good practice to clean up your cloud resources. You can delete the deployed Agent Engine instance to avoid any unexpected charges on your Google Cloud account."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remote_agent.delete()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfbF6BLIJLvm",
        "outputId": "b8f5bf7f-e243-42be-bad6-522977eceaca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.agent_engines:Delete Agent Engine backing LRO: projects/248711292256/locations/us-central1/operations/7006624739762372608\n",
            "INFO:vertexai.agent_engines:Agent Engine deleted. Resource name: projects/248711292256/locations/us-central1/reasoningEngines/6542204136429977600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create new session"
      ],
      "metadata": {
        "id": "fpkG58ywY5ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_engine = vertexai.agent_engines.get('projects/248711292256/locations/us-central1/reasoningEngines/5087541456789307392')"
      ],
      "metadata": {
        "id": "PsPL9bRiYzv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_engine.delete()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTFyxVOzY00n",
        "outputId": "7a6d7e7d-3880-4ab7-b854-bcfafd1163a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.agent_engines:Delete Agent Engine backing LRO: projects/248711292256/locations/us-central1/operations/6523728028936175616\n",
            "INFO:vertexai.agent_engines:Agent Engine deleted. Resource name: projects/248711292256/locations/us-central1/reasoningEngines/5087541456789307392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9xI1PTbRq7Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WCHFkjgq7aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sgpni5USq7cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pydantic import BaseModel\n",
        "# from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# def get_weather(city: str) -> str:\n",
        "#     \"\"\"Get weather for a given city.\"\"\"\n",
        "#     return f\"It's always sunny in {city}!\"\n",
        "\n",
        "# class WeatherResponse(BaseModel):\n",
        "#     conditions: str\n",
        "\n",
        "# agent = create_react_agent(\n",
        "#     model=\"gpt-4o-mini\",\n",
        "#     tools=[get_weather],\n",
        "#     response_format=WeatherResponse\n",
        "# )\n",
        "\n",
        "# response = agent.invoke(\n",
        "#     {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        "# )\n",
        "\n",
        "# response[\"structured_response\"]"
      ],
      "metadata": {
        "id": "dlyZujKOq7fL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f92fadd-27c0-46ff-cae6-447a986bdd9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-440936215.py:11: LangGraphDeprecatedSinceV10: create_react_agent has been moved to langchain.agents. Please update your import to 'from langchain.agents import create_agent'. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WeatherResponse(conditions=\"It's sunny in San Francisco!\")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}